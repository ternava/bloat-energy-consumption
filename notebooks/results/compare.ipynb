{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories():\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Get the directories of notebooks, data, and results\n",
    "    base_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "    data_dir = os.path.join(base_dir, \"data/repeat01\")\n",
    "    results_dir = os.path.join(base_dir, \"results\")\n",
    "\n",
    "    return base_dir, results_dir, data_dir\n",
    "\n",
    "# Store the three returned directories\n",
    "base_dir, results_dir, data_dir = get_directories()\n",
    "\n",
    "#print(base_dir, results_dir, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files stored in one of the data directories\n",
    "csv_files = [file for file in os.listdir(data_dir) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_01_ToyBox.csv' 'cut_02_BusyBox.csv' 'wc_01_GNU.csv'\n",
      " 'du_01_BusyBox.csv' 'mv_02_BusyBox.csv' 'cp_01_BusyBox.csv'\n",
      " 'wc_01_ToyBox.csv' 'cat_01_GNU.csv' 'wc_01_BusyBox.csv'\n",
      " 'cat_01_BusyBox.csv' 'mv_01_ToyBox.csv' 'cut_01_GNU.csv'\n",
      " 'cp_02_ToyBox.csv' 'sort_01_BusyBox.csv' 'cut_02_ToyBox.csv'\n",
      " 'du_02_GNU.csv' 'mv_02_ToyBox.csv' 'touch_01_ToyBox.csv'\n",
      " 'cat_02_ToyBox.csv' 'du_01_GNU.csv' 'mv_02_GNU.csv' 'cp_01_ToyBox.csv'\n",
      " 'cat_02_GNU.csv' 'cp_02_BusyBox.csv' 'ls_02_BusyBox.csv'\n",
      " 'touch_02_BusyBox.csv' 'sort_02_BusyBox.csv' 'touch_02_ToyBox.csv'\n",
      " 'ls_01_BusyBox.csv' 'touch_01_GNU.csv' 'du_02_ToyBox.csv' 'cp_01_GNU.csv'\n",
      " 'cut_01_BusyBox.csv' 'touch_02_GNU.csv' 'sort_02_ToyBox.csv'\n",
      " 'cat_01_ToyBox.csv' 'ls_02_GNU.csv' 'sort_01_ToyBox.csv' 'ls_01_GNU.csv'\n",
      " 'mv_01_GNU.csv' 'cat_02_BusyBox.csv' 'ls_01_ToyBox.csv' 'sort_01_GNU.csv'\n",
      " 'wc_02_BusyBox.csv' 'touch_01_BusyBox.csv' 'du_01_ToyBox.csv'\n",
      " 'wc_02_GNU.csv' 'cp_02_GNU.csv' 'ls_02_ToyBox.csv' 'du_02_BusyBox.csv'\n",
      " 'mv_01_BusyBox.csv' 'cut_02_GNU.csv' 'sort_02_GNU.csv' 'wc_02_ToyBox.csv']\n"
     ]
    }
   ],
   "source": [
    "def merge_all_csv_files(csv_files, data_dir):\n",
    "    merged_data = pd.DataFrame()\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        data = pd.read_csv(file_path)\n",
    "        data['File Name'] = file\n",
    "        merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def save_merged_data(merged_data, results_dir, output_filename):\n",
    "    merged_file_path = os.path.join(results_dir, output_filename)\n",
    "    merged_data.to_csv(merged_file_path, index=False)\n",
    "\n",
    "def print_merged_files(merged_data):\n",
    "    unique_files = merged_data['File Name'].unique()\n",
    "    print(unique_files)\n",
    "    \n",
    "merged_data_filename = 'merged_data.csv'\n",
    "\n",
    "# Merge all CSV files in the given path\n",
    "merged_data = merge_all_csv_files(csv_files, data_dir)\n",
    "\n",
    "# Save merged data to a new CSV file\n",
    "save_merged_data(merged_data, results_dir, merged_data_filename)\n",
    "\n",
    "# Print the names of merged files\n",
    "print_merged_files(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iteration;CORE;CPU;DRAM;DURATION;PSYS;UNCORE;EXIT_CODE', 'File Name'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'File Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/bloat-energy-consumption/venv/lib64/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/GitHub/bloat-energy-consumption/venv/lib64/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/bloat-energy-consumption/venv/lib64/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'File Name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m merged_data \u001b[39m=\u001b[39m read_merged_data(results_dir, merged_data_filename)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Update the combined data\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m updated_data \u001b[39m=\u001b[39m update_merged_data(merged_data)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Save the updated data to a new CSV file\u001b[39;00m\n\u001b[1;32m     39\u001b[0m save_updated_data(updated_data, results_dir, updated_data_filename)\n",
      "Cell \u001b[0;32mIn[184], line 10\u001b[0m, in \u001b[0;36mupdate_merged_data\u001b[0;34m(merged_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_merged_data\u001b[39m(merged_data):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Split the \"File Name\" column into three parts using underscore as separator\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     split_names \u001b[39m=\u001b[39m merged_data[\u001b[39m'\u001b[39;49m\u001b[39mFile Name\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, expand\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Create three new columns in the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     merged_data[\u001b[39m'\u001b[39m\u001b[39mProgram\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m split_names[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/bloat-energy-consumption/venv/lib64/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/bloat-energy-consumption/venv/lib64/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'File Name'"
     ]
    }
   ],
   "source": [
    "# It reads the CSV file with all merged data from other CSVs in the given path\n",
    "def read_merged_data(results_dir, filename):\n",
    "    merged_file_path = os.path.join(results_dir, filename)\n",
    "    merged_data = pd.read_csv(merged_file_path)\n",
    "    return merged_data\n",
    "\n",
    "# It slits the \"file name\" into three columns, to facilitate the future data processing\n",
    "def update_merged_data(merged_data):\n",
    "    # Split the \"File Name\" column into three parts using underscore as separator\n",
    "    split_names = merged_data['File Name'].str.split('_', n=2, expand=True)\n",
    "    \n",
    "    # Create three new columns in the dataset\n",
    "    merged_data['Program'] = split_names[0]\n",
    "    merged_data['TestNr'] = split_names[1]\n",
    "    merged_data['Utilities'] = split_names[2].str.replace(\".csv\", \"\")\n",
    "    \n",
    "    #merged_data[['Program', 'TestNr', 'Utilities']] = merged_data['File Name'].str.split('_', n=2, expand=True)\n",
    "    \n",
    "    # Remove the \"File Name\" column\n",
    "    merged_data = merged_data.drop('File Name', axis=1)\n",
    "    return merged_data\n",
    "\n",
    "# It saves the last CSV file, after update, to a new file\n",
    "def save_updated_data(merged_data, results_dir, filename):\n",
    "    updated_file_path = os.path.join(results_dir, filename)\n",
    "    merged_data.to_csv(updated_file_path, sep=';', index=False)\n",
    "    \n",
    "    \n",
    "merged_data_filename = 'merged_data.csv'\n",
    "updated_data_filename = 'updated_data.csv'\n",
    "\n",
    "# Read the combined data\n",
    "merged_data = read_merged_data(results_dir, merged_data_filename)\n",
    "\n",
    "# Update the combined data\n",
    "updated_data = update_merged_data(merged_data)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "save_updated_data(updated_data, results_dir, updated_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iteration;CORE;CPU;DRAM;DURATION;PSYS;UNCORE;EXIT_CODE', 'Program',\n",
      "       'TestNr', 'Utilities'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "updated_file_path = os.path.join(results_dir, \"updated_data.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(updated_file_path, delimiter=';')\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Sort the data based on the last three columns\n",
    "#df2 = df.sort_values('Program', ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
