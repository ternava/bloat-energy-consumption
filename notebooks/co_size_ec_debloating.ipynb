{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T14:58:03.500151019Z",
     "start_time": "2023-09-28T14:58:03.159461939Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T07:33:02.660803633Z",
     "start_time": "2023-08-02T07:33:02.639847815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the directory with the notebooks, including its subdirectories\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the directories with data and results\n",
    "data_dir = os.path.join(current_dir, \"data/debloating\")\n",
    "temporary_dir= os.path.join(current_dir, \"debloating_temporary_data\")\n",
    "results_dir = os.path.join(current_dir, \"debloating_results\")\n",
    "\n",
    "project_dir = os.path.dirname(current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:32:22.004898773Z",
     "start_time": "2023-08-01T12:32:21.321594800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat1.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat2.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat3.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat4.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat5.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat6.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat7.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat8.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat9.csv'.\n",
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_repeat10.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Using this function, the purpose is to cancatenate all generated measurements for each repeated experiment \n",
    "def concatenate_csv_files(directory_path, output_file):\n",
    "    file_list = os.listdir(directory_path)\n",
    "    csv_files = [file for file in file_list if file.endswith(\".csv\")]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = pd.read_csv(file_path, delimiter=';') # This is important, the delimiter\n",
    "        \n",
    "        filename_parts = file[:-4].split(\"_\")  # Split filename and remove \".csv\" extension        \n",
    "        df['Program'] = filename_parts[0].split(\"-\")[0]\n",
    "        df['TestNr'] = filename_parts[0].split(\"-\")[1]\n",
    "        df['Utilities'] = filename_parts[1]\n",
    "        dfs.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    concatenated_df.to_csv(output_file, index=False, sep=';')  # Use semicolon as delimiter\n",
    "\n",
    "    print(f\"CSV files concatenated and saved to '{output_file}'.\")\n",
    "\n",
    "for i in range(1,11):\n",
    "    # All data into files in the folder save in a new CSV file\n",
    "    input_data = os.path.join(data_dir, 'repeat'+str(i)+'/') # You need to change this for each new folder that you want to concatenate its files\n",
    "    output_data = os.path.join(results_dir, 'cat_repeat'+str(i)+'.csv') # You need to change this for each new folder given above\n",
    "\n",
    "    concatenate_csv_files(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:34:43.635112146Z",
     "start_time": "2023-08-01T12:34:43.583095778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files concatenated and saved to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/cat_all_repeats.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Using this function, the purpose is to concatenate all resulting files from all 5 executions\n",
    "def concatenate_allcsv_files(directory_path, output_file):\n",
    "    file_list = os.listdir(directory_path)\n",
    "    csv_files = [file for file in file_list if file.endswith(\".csv\")]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = pd.read_csv(file_path, delimiter=';') # This is important, the delimiter\n",
    "        df['Repetition'] = file  # Add filename as a new column\n",
    "        dfs.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    concatenated_df.to_csv(output_file, index=False, sep=';')  # Use semicolon as delimiter\n",
    "\n",
    "\n",
    "    print(f\"CSV files concatenated and saved to '{output_file}'.\")\n",
    "\n",
    "# Save in a new CSV file\n",
    "output_alldata = os.path.join(results_dir, 'cat_all_repeats.csv')\n",
    "\n",
    "concatenate_allcsv_files(temporary_dir, output_alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:35:59.472276959Z",
     "start_time": "2023-08-01T12:35:59.421881440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSYS values written to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/averagePSYS_all_repeats.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Now we need to find the average value of PSYS for all measurements of a program \n",
    "def calculate_average_PSYS(input_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file, delimiter=';')\n",
    "    \n",
    "    # Calculate average of 'PSYS' column for each unique combination of 'Program' and 'Utilities'\n",
    "    average_psys = df.groupby(['Program', 'Utilities'])['PSYS'].mean().reset_index()\n",
    "    \n",
    "    # Write the average values to a new file separated with a semicolon\n",
    "    average_psys.to_csv(output_file, index=False, sep=';')\n",
    "    \n",
    "    print(f\"Average PSYS values written to '{output_file}'.\")\n",
    "\n",
    "# Required inputs and outputs\n",
    "input_data = os.path.join(results_dir, 'cat_all_repeats.csv')\n",
    "output_data = os.path.join(results_dir, 'averagePSYS_all_repeats.csv')\n",
    "\n",
    "calculate_average_PSYS(input_data, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:36:24.361224627Z",
     "start_time": "2023-08-01T12:36:24.312940910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSYS and STD values written to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/averagePSYS_with_std_all_repeats.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_average_and_std_PSYS(input_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file, delimiter=';')\n",
    "    \n",
    "    # Convert 'PSYS' values from microjoules to joules\n",
    "    # df['PSYS'] = df['PSYS'] / 1000000  # 1 microjoule = 1e-6 joules\n",
    "\n",
    "    # Calculate average of 'PSYS' column for each unique combination of 'Program' and 'Utilities'\n",
    "    average_psys = df.groupby(['Program', 'Utilities'])['PSYS'].mean().reset_index()\n",
    "    average_psys['PSYS'] = average_psys['PSYS'].round(2)  # Round the average values to 2 decimals\n",
    "\n",
    "    # Calculate standard deviation of 'PSYS' column for each unique combination of 'Program' and 'Utilities'\n",
    "    std_psys = df.groupby(['Program', 'Utilities'])['PSYS'].std().reset_index()\n",
    "    std_psys['PSYS'] = std_psys['PSYS'].round(2)  # Round the standard deviation values to 2 decimals\n",
    "    \n",
    "    # Merge the average and standard deviation DataFrames based on 'Program' and 'Utilities' columns\n",
    "    result_df = pd.merge(average_psys, std_psys, on=['Program', 'Utilities'], suffixes=('_avg', '_std'))\n",
    "\n",
    "    # Write the average and standard deviation values to a new file separated with a semicolon\n",
    "    result_df.to_csv(output_file, index=False, sep=';')\n",
    "\n",
    "    print(f\"Average PSYS and STD values written to '{output_file}'.\")\n",
    "\n",
    "# Required inputs and outputs\n",
    "input_data = os.path.join(results_dir, 'cat_all_repeats.csv')\n",
    "output_data = os.path.join(results_dir, 'averagePSYS_with_std_all_repeats.csv')\n",
    "\n",
    "calculate_average_and_std_PSYS(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:39:06.749375865Z",
     "start_time": "2023-08-01T16:39:06.727377952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Program  PSYS_avg_bloated  PSYS_avg_chisel  PSYS_avg_cov  \\\n",
      "0         date         287914.25        284160.70     284352.85   \n",
      "1         grep         317086.10        457307.85     315260.85   \n",
      "2         gzip         334755.55        549107.40     331841.20   \n",
      "3        mkdir         289446.30        287996.70     287615.30   \n",
      "4  printokens2         282000.05        281304.40     285134.15   \n",
      "5          sed         285567.55        288768.75     290404.50   \n",
      "\n",
      "   PSYS_avg_debop  PSYS_std_bloated  PSYS_std_chisel  PSYS_std_cov  \\\n",
      "0       284331.55          14287.28          9876.11       9444.81   \n",
      "1       315404.45          19601.77         20620.21      17533.55   \n",
      "2       331715.95          12876.30         22471.20      16107.98   \n",
      "3       287520.60          15787.29         13904.30      11606.01   \n",
      "4       281771.20           8763.53         12498.42      21416.34   \n",
      "5       287371.10           9693.99         15200.94      21546.36   \n",
      "\n",
      "   PSYS_std_debop  \n",
      "0         9677.53  \n",
      "1        18249.06  \n",
      "2         8457.84  \n",
      "3        11476.92  \n",
      "4        13469.48  \n",
      "5        13604.23  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Required inputs and outputs\n",
    "input_data = os.path.join(results_dir, 'averagePSYS_with_std_all_repeats.csv')\n",
    "output_data = os.path.join(results_dir, 'averagePSYS_with_std_all_repeats_pivot.csv')\n",
    "\n",
    "data = pd.read_csv(input_data, sep=';')\n",
    "\n",
    "# Pivot the data to create new columns for GNU, ToyBox, and BusyBox\n",
    "pivoted_data = data.pivot(index='Program', columns='Utilities', values=['PSYS_avg', 'PSYS_std'])\n",
    "\n",
    "# Flatten the multi-level column index\n",
    "pivoted_data.columns = [f'{col[0]}_{col[1]}' for col in pivoted_data.columns]\n",
    "\n",
    "# Reset the index to make 'Program' a regular column again\n",
    "pivoted_data = pivoted_data.reset_index()\n",
    "\n",
    "pivoted_data.to_csv(output_data, index=False, sep=';')\n",
    "\n",
    "# Display the reorganized data\n",
    "print(pivoted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:39:18.065352866Z",
     "start_time": "2023-08-01T16:39:18.048554493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSYS and DURATION values written to '/home/rlefeuvr/Documents/Workspace/Debloat/bloat-energy-consumption/notebooks/debloating_results/averageDURATION_all_repeats.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Now we need to find the average value of PSYS and DURATION for all measurements of a program \n",
    "def calculate_average_PSYS_DURATION(input_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file, delimiter=';')\n",
    "    \n",
    "    # Calculate average of 'PSYS' column for each unique combination of 'Program' and 'Utilities'\n",
    "    average_psys = df.groupby(['Program', 'Utilities'])['PSYS'].mean().reset_index()\n",
    "    \n",
    "    # Calculate average of 'DURATION' column for each unique combination of 'Program' and 'Utilities'\n",
    "    average_duration = df.groupby(['Program', 'Utilities'])['DURATION'].mean().reset_index()\n",
    "    \n",
    "    # Merge the two DataFrames based on 'Program' and 'Utilities'\n",
    "    merged_df = pd.merge(average_psys, average_duration, on=['Program', 'Utilities'])\n",
    "    \n",
    "    # Write the average values to a new file separated with a semicolon\n",
    "    merged_df.to_csv(output_file, index=False, sep=';')\n",
    "    \n",
    "    print(f\"Average PSYS and DURATION values written to '{output_file}'.\")\n",
    "\n",
    "# Required inputs and outputs\n",
    "input_data = os.path.join(results_dir, 'cat_all_repeats.csv')\n",
    "output_data = os.path.join(results_dir, 'averageDURATION_all_repeats.csv')\n",
    "\n",
    "calculate_average_PSYS_DURATION(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:44:54.845555535Z",
     "start_time": "2023-08-01T16:44:54.799203658Z"
    }
   },
   "outputs": [],
   "source": [
    "# After we need to put together the PSYS and Size values for each program in each of the 3 different implementations\n",
    "\n",
    "csv_with_ec = os.path.join(results_dir, 'averagePSYS_all_repeats.csv')\n",
    "csv_with_size = os.path.join(results_dir, 'debloat_experiments_size.csv') \n",
    "output_file = os.path.join(results_dir, 'all_ec_bsize.csv')\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv(csv_with_ec, delimiter=';')\n",
    "# Read the second CSV file\n",
    "df2 = pd.read_csv(csv_with_size, delimiter=';')\n",
    "df1['Program'] = df1['Program'].replace('printokens2', 'printtokens2')\n",
    "# Merge the two DataFrames based on 'Program' column\n",
    "#NOTE TODO FIX THE MANUAL PROCESSING NEEDED : REPLACE PRINTOKEN TO PRINTTOKEN\n",
    "merged_df = pd.merge(df1, df2, on='Program')\n",
    "# Add the 'Size' values for each 'Utilities' based on the Program and fillna with 0 if not found\n",
    "df1['Size'] = merged_df.apply(lambda row: row[row['Utilities']], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file, including 'Utilities' and 'PSYS' columns\n",
    "df1.to_csv(output_file, index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Program Utilities       PSYS    Size\n",
      "20     sed   bloated  285567.55  174472\n",
      "21     sed    chisel  288768.75  155224\n",
      "22     sed       cov  290404.50   87856\n",
      "23     sed     debop  287371.10   85772\n"
     ]
    }
   ],
   "source": [
    "df1['Program'] = df1['Program'].str.strip()\n",
    "print(df1[df1['Program'].str.contains('sed', na=False)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corelation between PSYS and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:46:42.948780154Z",
     "start_time": "2023-08-01T16:46:42.925837499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Program Utilities       PSYS    Size\n",
      "0           date   bloated  287914.25   94240\n",
      "1           date    chisel  284160.70   27952\n",
      "2           date       cov  284352.85   37536\n",
      "3           date     debop  284331.55   37536\n",
      "4           grep   bloated  317086.10  162640\n",
      "5           grep    chisel  457307.85  111664\n",
      "6           grep       cov  315260.85   87656\n",
      "7           grep     debop  315404.45   87664\n",
      "8           gzip   bloated  334755.55  104152\n",
      "9           gzip    chisel  549107.40   91720\n",
      "10          gzip       cov  331841.20   56616\n",
      "11          gzip     debop  331715.95   56624\n",
      "12         mkdir   bloated  289446.30   49360\n",
      "13         mkdir    chisel  287996.70   19696\n",
      "14         mkdir       cov  287615.30   23392\n",
      "15         mkdir     debop  287520.60   23392\n",
      "16  printtokens2   bloated  282000.05   21176\n",
      "17  printtokens2    chisel  281304.40   21168\n",
      "18  printtokens2       cov  285134.15   21184\n",
      "19  printtokens2     debop  281771.20   21192\n",
      "20           sed   bloated  285567.55  174472\n",
      "21           sed    chisel  288768.75  155224\n",
      "22           sed       cov  290404.50   87856\n",
      "23           sed     debop  287371.10   85772\n",
      "           Spearman Correlation   p-value\n",
      "Utilities                                \n",
      "bloated                0.314286  0.544093\n",
      "chisel                 0.600000  0.208000\n",
      "cov                    0.600000  0.208000\n",
      "debop                  0.600000  0.208000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758743/213553716.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = df.groupby('Utilities').apply(lambda x: spearmanr(x['PSYS'], x['Size']))\n"
     ]
    }
   ],
   "source": [
    "# Now we want to calculate the Peason correlation between the EC and Size \n",
    "# for all utilities in each 3 different implementations\n",
    "\n",
    "# The Spearman correlation with the p-value\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "input_data = os.path.join(results_dir, 'all_ec_bsize.csv')\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_data, delimiter=\";\")\n",
    "print(df)\n",
    "# Convert 'PSYS' and 'Size' columns to numeric\n",
    "df['PSYS'] = pd.to_numeric(df['PSYS'])\n",
    "df['Size'] = pd.to_numeric(df['Size'])\n",
    "\n",
    "# Calculate Spearman correlation and p-value for each 'Utilities' separately\n",
    "results = df.groupby('Utilities').apply(lambda x: spearmanr(x['PSYS'], x['Size']))\n",
    "\n",
    "# Extract the correlation coefficients and p-values\n",
    "correlations = results.apply(lambda x: x.correlation)\n",
    "p_values = results.apply(lambda x: x.pvalue)\n",
    "\n",
    "# Combine correlations and p-values into a DataFrame\n",
    "spearman_df_size = pd.DataFrame({'Spearman Correlation': correlations, 'p-value': p_values})\n",
    "\n",
    "# Print the correlations and p-values\n",
    "print(spearman_df_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_correlation_dfs(spearman: pd.DataFrame, pearson: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Rename columns for clarity\n",
    "    spearman = spearman.rename(columns={\n",
    "        'Spearman Correlation': 'Spearman',\n",
    "        'p-value': 'p-value_s'\n",
    "    })\n",
    "    \n",
    "    pearson = pearson.rename(columns={\n",
    "        'Pearson Correlation': 'Pearson',\n",
    "        'p-value': 'p-value_p'\n",
    "    })\n",
    "    \n",
    "    # Merge on index (Utilities)\n",
    "    merged_df = spearman.merge(pearson, left_index=True, right_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "def print_latex_df(df: pd.DataFrame, filename: str = \"output.tex\"):\n",
    "    # Define column format: 'l' for first column, a separator '|', and 'c' for the rest\n",
    "    column_format = \"l| \" + \"c \" * (len(df.columns))\n",
    "\n",
    "    # Convert DataFrame to LaTeX without index and with correct format\n",
    "    latex_str = df.to_latex(\n",
    "        index=True,  # No row index\n",
    "        float_format=lambda x: f\"{x:.2g}\",  # Limit float precision\n",
    "        column_format=column_format.strip(),  # Ensure clean formatting\n",
    "        escape=False  # Prevent LaTeX escaping\n",
    "    )\n",
    "\n",
    "    # Remove any unwanted extra row\n",
    "    latex_str = latex_str.replace(\"\\\\toprule\\n &\", \"\\\\toprule\\n\")\n",
    "    latex_str = latex_str.replace(\"\\\\midrule\\n &\", \"\\\\midrule\\n\") \n",
    "    latex_str = latex_str.replace(\"Utilities &  &  &  &  \\\\\\\\\", \"\")  \n",
    "    # Make headers bold dynamically\n",
    "\n",
    "    headers = \" & \".join(f\"{col}\" for col in df.columns)\n",
    "    headers = df.index.name+\" & \"+headers\n",
    "    latex_str = latex_str.replace(\" & \".join(df.columns), headers)\n",
    "    print(latex_str)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(latex_str)\n",
    "\n",
    "\n",
    "\n",
    "def print_latex_compact_df(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    label1: str = \"DF1\",\n",
    "    label2: str = \"DF2\",\n",
    "    filename: str = \"compact_output.tex\"\n",
    "):\n",
    "    def format_number(x):\n",
    "        try:\n",
    "            x = float(x)\n",
    "            if abs(x) <= 1e-3 and x != 0.0:\n",
    "                return f\"{x:.2e}\"  # scientific notation\n",
    "            else:\n",
    "                return f\"{x:.3f}\"\n",
    "        except:\n",
    "            return str(x)\n",
    "\n",
    "    def format_df(df):\n",
    "        df_formatted = df.copy()\n",
    "        for idx, row in df.iterrows():\n",
    "            \n",
    "            if float(row.iloc[1]) > 0.05:\n",
    "                df_formatted.loc[idx] = row.apply(lambda x: f\"\\\\hatchedCell{{{format_number(x)}}}\")\n",
    "            else:\n",
    "                df_formatted.loc[idx] = row.apply(lambda x: format_number(x))\n",
    "        return df_formatted\n",
    "\n",
    "    df1_fmt = format_df(df1)\n",
    "    df2_fmt = format_df(df2)\n",
    "\n",
    "    # Add \\textit{} around the index (utility names)\n",
    "    df1_fmt.index = [f\"\\\\textit{{{i}}}\" for i in df1_fmt.index]\n",
    "    df2_fmt.index = [f\"\\\\textit{{{i}}}\" for i in df2_fmt.index]\n",
    "\n",
    "    combined = pd.concat([df1_fmt, df2_fmt], axis=1, keys=[label1, label2])\n",
    "    combined.columns = pd.MultiIndex.from_tuples(combined.columns)\n",
    "\n",
    "    latex_str = combined.to_latex(\n",
    "        escape=False,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        multirow=True,\n",
    "        index=True,\n",
    "        column_format=\"l|cc||cc||cc\"\n",
    "    )\n",
    "\n",
    "    print(latex_str)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(latex_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:46:44.708695550Z",
     "start_time": "2023-08-01T16:46:44.662213896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pearson Correlation   p-value\n",
      "Utilities                               \n",
      "bloated               0.334806  0.516557\n",
      "chisel                0.386750  0.448800\n",
      "cov                   0.465291  0.352430\n",
      "debop                 0.463585  0.354437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758743/2946106904.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = df.groupby('Utilities').apply(lambda x: pearsonr(x['PSYS'], x['Size']))\n"
     ]
    }
   ],
   "source": [
    "# The Pearson correlation with the p-value\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "input_data = os.path.join(results_dir, 'all_ec_bsize.csv')\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_data, delimiter=\";\")\n",
    "\n",
    "# Convert 'PSYS' and 'Size' columns to numeric\n",
    "df['PSYS'] = pd.to_numeric(df['PSYS'])\n",
    "df['Size'] = pd.to_numeric(df['Size'])\n",
    "\n",
    "# Calculate Spearman correlation and p-value for each 'Utilities' separately\n",
    "results = df.groupby('Utilities').apply(lambda x: pearsonr(x['PSYS'], x['Size']))\n",
    "\n",
    "# Extract the correlation coefficients and p-values\n",
    "correlations = results.apply(lambda x: x.correlation)\n",
    "p_values = results.apply(lambda x: x.pvalue)\n",
    "\n",
    "# Combine correlations and p-values into a DataFrame\n",
    "pearson_df = pd.DataFrame({'Pearson Correlation': correlations, 'p-value': p_values})\n",
    "\n",
    "# Print the correlations and p-values\n",
    "print(pearson_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l| c c c c}\n",
      "\\toprule\n",
      " Utilities & Spearman & p-value_s & Pearson & p-value_p \\\\\n",
      "\n",
      "\\midrule\n",
      "bloated & 0.31 & 0.54 & 0.33 & 0.52 \\\\\n",
      "chisel & 0.6 & 0.21 & 0.39 & 0.45 \\\\\n",
      "cov & 0.6 & 0.21 & 0.47 & 0.35 \\\\\n",
      "debop & 0.6 & 0.21 & 0.46 & 0.35 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df_size = merge_correlation_dfs(spearman_df_size, pearson_df)\n",
    "print_latex_df(merged_df_size, filename=os.path.join(results_dir, \"correlation_size.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between PSYS and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T10:37:22.979045126Z",
     "start_time": "2023-08-02T10:37:22.443006103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Spearman Correlation   p-value\n",
      "Utilities                                \n",
      "bloated                1.000000  0.000000\n",
      "chisel                 1.000000  0.000000\n",
      "cov                    0.885714  0.018845\n",
      "debop                  0.942857  0.004805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758743/1557892229.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = df.groupby('Utilities').apply(lambda x: spearmanr(x['PSYS'], x['DURATION']))\n"
     ]
    }
   ],
   "source": [
    "# Now we want to calculate the Peason correlation between the EC and Duration \n",
    "# for all utilities in each 3 different implementations\n",
    "\n",
    "# The Spearman correlation with the p-value\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "input_data = os.path.join(results_dir, 'averageDURATION_all_repeats.csv')\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_data, delimiter=\";\")\n",
    "\n",
    "# Convert 'PSYS' and 'Size' columns to numeric\n",
    "df['PSYS'] = pd.to_numeric(df['PSYS'])\n",
    "df['DURATION'] = pd.to_numeric(df['DURATION'])\n",
    "\n",
    "# Calculate Spearman correlation and p-value for each 'Utilities' separately\n",
    "results = df.groupby('Utilities').apply(lambda x: spearmanr(x['PSYS'], x['DURATION']))\n",
    "\n",
    "# Extract the correlation coefficients and p-values\n",
    "correlations = results.apply(lambda x: x.correlation)\n",
    "p_values = results.apply(lambda x: x.pvalue)\n",
    "\n",
    "# Combine correlations and p-values into a DataFrame\n",
    "spearman_df_perf = pd.DataFrame({'Spearman Correlation': correlations, 'p-value': p_values})\n",
    "\n",
    "# Print the correlations and p-values\n",
    "print(spearman_df_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T10:37:33.239865769Z",
     "start_time": "2023-08-02T10:37:32.910703543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pearson Correlation       p-value\n",
      "Utilities                                   \n",
      "bloated               0.998515  3.304415e-06\n",
      "chisel                0.999928  7.679358e-09\n",
      "cov                   0.996567  1.765990e-05\n",
      "debop                 0.998403  3.822160e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758743/95036515.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = df.groupby('Utilities').apply(lambda x: pearsonr(x['PSYS'], x['DURATION']))\n"
     ]
    }
   ],
   "source": [
    "# The Pearson correlation with the p-value\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "input_data = os.path.join(results_dir, 'averageDURATION_all_repeats.csv')\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_data, delimiter=\";\")\n",
    "\n",
    "# Convert 'PSYS' and 'Size' columns to numeric\n",
    "df['PSYS'] = pd.to_numeric(df['PSYS'])\n",
    "df['DURATION'] = pd.to_numeric(df['DURATION'])\n",
    "\n",
    "# Calculate Spearman correlation and p-value for each 'Utilities' separately\n",
    "results = df.groupby('Utilities').apply(lambda x: pearsonr(x['PSYS'], x['DURATION']))\n",
    "\n",
    "# Extract the correlation coefficients and p-values\n",
    "correlations = results.apply(lambda x: x.correlation)\n",
    "p_values = results.apply(lambda x: x.pvalue)\n",
    "\n",
    "# Combine correlations and p-values into a DataFrame\n",
    "pearson_df = pd.DataFrame({'Pearson Correlation': correlations, 'p-value': p_values})\n",
    "\n",
    "# Print the correlations and p-values\n",
    "print(pearson_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l| c c c c}\n",
      "\\toprule\n",
      " Utilities & Spearman & p-value_s & Pearson & p-value_p \\\\\n",
      "\n",
      "\\midrule\n",
      "bloated & 1 & 0 & 1 & 3.3e-06 \\\\\n",
      "chisel & 1 & 0 & 1 & 7.7e-09 \\\\\n",
      "cov & 0.89 & 0.019 & 1 & 1.8e-05 \\\\\n",
      "debop & 0.94 & 0.0048 & 1 & 3.8e-06 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df_perf = merge_correlation_dfs(spearman_df_perf, pearson_df)\n",
    "print_latex_df(merged_df_perf, filename=os.path.join(results_dir, \"correlation_perf.tex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l|cc||cc||cc}\n",
      "\\toprule\n",
      " & \\multicolumn{2}{c}{Energy/Perf} & \\multicolumn{2}{c}{Energy/Binary Size} \\\\\n",
      " & Spearman Correlation & p-value & Spearman Correlation & p-value \\\\\n",
      "\\midrule\n",
      "\\textit{bloated} & 1.000 & 0.000 & \\hatchedCell{0.314} & \\hatchedCell{0.544} \\\\\n",
      "\\textit{chisel} & 1.000 & 0.000 & \\hatchedCell{0.600} & \\hatchedCell{0.208} \\\\\n",
      "\\textit{cov} & 0.886 & 0.019 & \\hatchedCell{0.600} & \\hatchedCell{0.208} \\\\\n",
      "\\textit{debop} & 0.943 & 0.005 & \\hatchedCell{0.600} & \\hatchedCell{0.208} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758743/2417501787.py:68: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_formatted.loc[idx] = row.apply(lambda x: format_number(x))\n",
      "/tmp/ipykernel_758743/2417501787.py:68: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_formatted.loc[idx] = row.apply(lambda x: format_number(x))\n",
      "/tmp/ipykernel_758743/2417501787.py:66: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '\\hatchedCell{0.314}' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_formatted.loc[idx] = row.apply(lambda x: f\"\\\\hatchedCell{{{format_number(x)}}}\")\n",
      "/tmp/ipykernel_758743/2417501787.py:66: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '\\hatchedCell{0.544}' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_formatted.loc[idx] = row.apply(lambda x: f\"\\\\hatchedCell{{{format_number(x)}}}\")\n"
     ]
    }
   ],
   "source": [
    "print_latex_compact_df(spearman_df_perf,spearman_df_size, label1=\"Energy/Perf\", label2=\"Energy/Binary Size\", filename=os.path.join(results_dir, \"correlation_compact_debloat.tex\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
